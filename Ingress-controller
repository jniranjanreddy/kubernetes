```
1. MetalLB
2. NodePort
3. HostNetwork
4. LoadBalancer
```
```
hostNetwork: true
The hostNetwork setting applies to the Kubernetes pods. When a pod is configured with hostNetwork: true, 
the applications running in such a pod can directly see the network interfaces of the host machine where the pod was started. 
An application that is configured to listen on all network interfaces will in turn be accessible on all network interfaces of the host machine. 
Here is an example definition of a pod that uses host networking:

cat influxdb-hostnetwork.yml

apiVersion: v1
kind: Pod
metadata:
  name: influxdb
spec:
  hostNetwork: true
  containers:
    - name: influxdb
      image: influxdb
      
You can start the pod with the following command:
$ kubectl create -f influxdb-hostnetwork.yml

You can check that the InfluxDB application is running with:
$ curl -v http://kubenode01.example.com:8086/ping

Remember to replace the host name in the above URL with the host name or IP address of the Kubernetes node where your pod has been scheduled to run. 
InfluxDB will respond with HTTP 204 No Content when working properly.

Note that every time the pod is restarted Kubernetes can reschedule the pod onto a different node and so the application will change its IP address. 
Besides that two applications requiring the same port cannot run on the same node. This can lead to port conflicts when the number of applications running on the cluster grows. 
On top of that, creating a pod with hostNetwork: true on OpenShift is a privileged operation. 
For these reasons, the host networking is not a good way to make your applications accessible from outside of the cluster.

What is the host networking good for? For cases where a direct access to the host networking is required. For example, 
the Kubernetes networking plugin Flannel can be deployed as a daemon set on all nodes of the Kubernetes cluster. Due to hostNetwork: true 
the Flannel has full control of the networking on every node in the cluster allowing it to manage the overlay network to which the pods with hostNetwork: false are connected to.
#========================================================================================================================================
--------
hostPort
--------
The hostPort setting applies to the Kubernetes containers. The container port will be exposed to the external network at <hostIP>:<hostPort>, 
where the hostIP is the IP address of the Kubernetes node where the container is running and the hostPort is the port requested by the user. 
Here comes a sample pod definition:

cat influxdb-hostport.yml
apiVersion: v1
kind: Pod
metadata:
  name: influxdb
spec:
  containers:
    - name: influxdb
      image: influxdb
      ports:
        - containerPort: 8086
          hostPort: 8086

The hostPort feature allows to expose a single container port on the host IP. 
Using the hostPort to expose an application to the outside of the Kubernetes cluster has the same drawbacks as the hostNetwork approach discussed in the previous section. 
The host IP can change when the container is restarted, two containers using the same hostPort cannot be scheduled on the same node and the usage of the hostPort is 
considered a privileged operation on OpenShift.

What is the hostPort used for? For example, the nginx based Ingress controller is deployed as a set of containers running on top of Kubernetes. 
These containers are configured to use hostPorts 80 and 443 to allow the inbound traffic on these ports from the outside of the Kubernetes cluster.
#================================================================================================================================================
--------
NodePort
--------
The NodePort setting applies to the Kubernetes services. By default Kubernetes services are accessible at the ClusterIP 
which is an internal IP address reachable from inside of the Kubernetes cluster only. The ClusterIP enables the applications running within 
the pods to access the service. To make the service accessible from outside of the cluster a user can create a service of type NodePort. At first, 
let’s review the definition of the pod that we’ll expose using a NodePort service:

cat influxdb-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: influxdb
  labels:
    name: influxdb
spec:
  containers:
    - name: influxdb
      image: influxdb
      ports:
        - containerPort: 8086
        
 When creating a NodePort service, the user can specify a port from the range 30000-32767, and each Kubernetes node will proxy that port to the pods selected by the service.
 A sample definition of a NodePort service looks as follows:

kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: NodePort
  ports:
    - port: 8086
      nodePort: 30000
  selector:
    name: influxdb

Note that on OpenShift more privileges are required to create a NodePort service. After the service has been created, 
the kube-proxy component that runs on each node of the Kubernetes cluster and listens on all network interfaces is instructed to accept connections on port 30000. 
The incoming traffic is forwarded by the kube-proxy to the selected pods in a round-robin fashion. 
You should be able to access the InfluxDB application from outside of the cluster using the command:

$ curl -v http://kubenode01.example.com:30000/ping
The NodePort service represents a static endpoint through which the selected pods can be reached. 
If you prefer serving your application on a different port than the 30000-32767 range, 
you can deploy an external load balancer in front of the Kubernetes nodes and forward the traffic to the NodePort on each of the Kubernetes nodes. 
This gives you an extra resiliency for the case that some of the Kubernetes nodes becomes unavailable, too. 
If you’re hosting your Kubernetes cluster on one of the supported cloud providers like AWS, Azure or GCE, Kubernetes can provision an external load balancer for you.
We’ll take a look at how to do it in the next section.
#=========================================================================================================
------------
LoadBalancer
------------
The LoadBalancer setting applies to the Kubernetes service. In order to be able to create a service of type LoadBalancer, 
a cloud provider has to be enabled in the configuration of the Kubernetes cluster. As of version 1.6, Kubernetes can provision load balancers on AWS, Azure,
CloudStack, GCE and OpenStack. Here is an example definition of the LoadBalancer service:

cat influxdb-loadbalancer.yml

kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: LoadBalancer
  ports:
    - port: 8086
  selector:
    name: influxdb

Let’s take a look at what Kubernetes created for us:

$ kubectl get svc influxdb
NAME       CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s

In the command output we can read that the influxdb service is internally reachable at the ClusterIP 10.97.121.42. Next, Kubernetes allocated a NodePort 30051. Because we didn’t specify a desired NodePort number, Kubernetes picked one for us. We can check the reachability of the InfluxDB application through the NodePort with the command:

1
$ curl -v http://kubenode01.example.com:30051/ping
Finally, Kubernetes reached out to the cloud provider to provision a load balancer. The VIP of the load balancer is 10.13.242.236 as it is shown in the command output. Now we can access the InfluxDB application through the load balancer like this:

1
$ curl -v http://10.13.242.236:8086/ping
```
